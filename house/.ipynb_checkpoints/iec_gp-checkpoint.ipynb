{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  Intelligent energy component contains the IEC class, that includes several algorithms\n",
    "     for predicting consumption of a house, given historical data. It also contains an IECTester\n",
    "     class that can be used to test and provide results on multiple IEC runs \"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import scipy.ndimage.filters\n",
    "import scipy.signal\n",
    "from scipy import spatial\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import GPy\n",
    "\n",
    "try:\n",
    "    from .easing import *\n",
    "except:\n",
    "    from easing import *\n",
    "\n",
    "cons_col = 'House Consumption'\n",
    "\n",
    "\n",
    "class NoSimilarMomentsFound(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate the cosine similarity between\n",
    "    two non-zero vectors of equal length (https://en.wikipedia.org/wiki/Cosine_similarity)\n",
    "    \"\"\"\n",
    "    return 1.0 - spatial.distance.cosine(a, b)\n",
    "\n",
    "\n",
    "def baseline_similarity(a, b, filter=True):\n",
    "    if filter is True:\n",
    "        similarity = -mean_squared_error(gauss_filt(a, 201), gauss_filt(b, 201)) ** 0.5\n",
    "    else:\n",
    "        similarity = -mean_squared_error(a, b) ** 0.5\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def advanced_similarity(a, b):\n",
    "    sigma = 10\n",
    "\n",
    "    base_similarity = baseline_similarity(a, b)\n",
    "\n",
    "    high_pass_a = highpass_filter(a)\n",
    "    high_pass_b = highpass_filter(b)\n",
    "\n",
    "    high_pass_a = scipy.ndimage.filters.gaussian_filter1d(high_pass_a, sigma)\n",
    "    high_pass_b = scipy.ndimage.filters.gaussian_filter1d(high_pass_b, sigma)\n",
    "\n",
    "    highpass_similarity = -mean_squared_error(high_pass_a, high_pass_b)\n",
    "\n",
    "    return base_similarity + highpass_similarity\n",
    "\n",
    "\n",
    "def mins_in_day(timestamp):\n",
    "    return timestamp.hour * 60 + timestamp.minute\n",
    "\n",
    "\n",
    "def find_similar_days(training_data, observation_length, k, interval, method=cosine_similarity):\n",
    "    now = training_data.index[-1]\n",
    "    timezone = training_data.index.tz\n",
    "\n",
    "    # Find moments in our dataset that have the same hour/minute and is_weekend() == weekend.\n",
    "    # Those are the indexes of those moments in TrainingData\n",
    "\n",
    "    min_time = training_data.index[0] + timedelta(minutes=observation_length)\n",
    "\n",
    "    selector = (\n",
    "        (training_data.index.minute == now.minute) &\n",
    "        (training_data.index.hour == now.hour) &\n",
    "        (training_data.index > min_time)\n",
    "    )\n",
    "    similar_moments = training_data[selector][:-1].tz_convert('UTC')\n",
    "\n",
    "    if similar_moments.empty:\n",
    "        raise NoSimilarMomentsFound\n",
    "\n",
    "    training_data = training_data.tz_convert('UTC')\n",
    "\n",
    "    last_day_vector = (training_data\n",
    "                       .tail(observation_length)\n",
    "                       .resample(timedelta(minutes=interval))\n",
    "                       .sum()\n",
    "                       )\n",
    "\n",
    "    obs_td = timedelta(minutes=observation_length)\n",
    "\n",
    "    similar_moments['Similarity'] = [\n",
    "        method(\n",
    "            last_day_vector.as_matrix(columns=[cons_col]),\n",
    "            training_data[i - obs_td:i].resample(timedelta(minutes=interval)).sum().as_matrix(columns=[cons_col])\n",
    "        ) for i in similar_moments.index\n",
    "    ]\n",
    "\n",
    "    indexes = (similar_moments\n",
    "               .sort_values('Similarity', ascending=False)\n",
    "               .head(k)\n",
    "               .index\n",
    "               .tz_convert(timezone))\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def lerp(x, y, alpha):\n",
    "    assert x.shape == y.shape and x.shape == alpha.shape  # shapes must be equal\n",
    "\n",
    "    x *= 1 - alpha\n",
    "    y *= alpha\n",
    "\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def med_filt(x, k=201):\n",
    "    \"\"\"Apply a length-k median filter to a 1D array x.\n",
    "    Boundaries are extended by repeating endpoints.\n",
    "    \"\"\"\n",
    "    if x.ndim > 1:\n",
    "        x = np.squeeze(x)\n",
    "    med = np.median(x)\n",
    "    assert k % 2 == 1, \"Median filter length must be odd.\"\n",
    "    assert x.ndim == 1, \"Input must be one-dimensional.\"\n",
    "    k2 = (k - 1) // 2\n",
    "    y = np.zeros((len(x), k), dtype=x.dtype)\n",
    "    y[:, k2] = x\n",
    "    for i in range(k2):\n",
    "        j = k2 - i\n",
    "        y[j:, i] = x[:-j]\n",
    "        y[:j, i] = x[0]\n",
    "        y[:-j, -(i + 1)] = x[j:]\n",
    "        y[-j:, -(i + 1)] = med\n",
    "    return np.median(y, axis=1)\n",
    "\n",
    "\n",
    "def gauss_filt(x, k=201):\n",
    "    \"\"\"Apply a length-k gaussian filter to a 1D array x.\n",
    "    Boundaries are extended by repeating endpoints.\n",
    "    \"\"\"\n",
    "    if x.ndim > 1:\n",
    "        x = np.squeeze(x)\n",
    "    med = np.median(x)\n",
    "    assert k % 2 == 1, \"mean filter length must be odd.\"\n",
    "    assert x.ndim == 1, \"Input must be one-dimensional.\"\n",
    "    k2 = (k - 1) // 2\n",
    "    y = np.zeros((len(x), k), dtype=x.dtype)\n",
    "    y[:, k2] = x\n",
    "    for i in range(k2):\n",
    "        j = k2 - i\n",
    "        y[j:, i] = x[:-j]\n",
    "        y[:j, i] = x[0]\n",
    "        y[:-j, -(i + 1)] = x[j:]\n",
    "        y[-j:, -(i + 1)] = med\n",
    "    return np.mean(y, axis=1)\n",
    "\n",
    "\n",
    "def calc_baseline(training_data, similar_moments,\n",
    "                  prediction_window, half_window=100, method=gauss_filt, interp_range=200):\n",
    "    prediction_window_in_mins = prediction_window\n",
    "    if type(prediction_window) is not timedelta:\n",
    "        prediction_window = timedelta(minutes=prediction_window)\n",
    "\n",
    "    k = len(similar_moments)\n",
    "\n",
    "    r = np.zeros((prediction_window_in_mins + 1, 1))\n",
    "    for i in similar_moments:\n",
    "        r += (1 / k) * training_data[i:i + prediction_window].rolling(window=half_window * 2, center=True,\n",
    "                                                                      min_periods=1).mean().as_matrix()\n",
    "    baseline = np.squeeze(r)\n",
    "\n",
    "    recent_baseline = training_data[-2 * half_window:-1].mean()[cons_col]\n",
    "\n",
    "    if interp_range > 0:\n",
    "        baseline[:interp_range] = lerp(np.repeat(recent_baseline, interp_range),\n",
    "                                       baseline[:interp_range],\n",
    "                                       np.arange(interp_range) / interp_range)\n",
    "\n",
    "    return baseline\n",
    "\n",
    "\n",
    "def calc_baseline_dumb(training_data, similar_moments,\n",
    "                       prediction_window):\n",
    "    if type(prediction_window) is not timedelta:\n",
    "        prediction_window = timedelta(minutes=prediction_window)\n",
    "\n",
    "    k = len(similar_moments)\n",
    "\n",
    "    r = np.zeros((49, 1))\n",
    "    for i in similar_moments:\n",
    "        similar_day = (1 / k) * training_data[i:i + prediction_window].resample(timedelta(minutes=15)).mean()\n",
    "        similar_day = similar_day[0:49]\n",
    "        r += similar_day\n",
    "        # r += (1 / k) * training_data[i:i + prediction_window].as_matrix\n",
    "\n",
    "    baseline = np.squeeze(r)\n",
    "\n",
    "    b = pd.DataFrame(baseline).set_index(pd.TimedeltaIndex(freq='15T', start=0, periods=49)).resample(\n",
    "        timedelta(minutes=1)).ffill()\n",
    "    baseline = np.squeeze(b.as_matrix())\n",
    "    baseline = np.concatenate((baseline, np.atleast_1d(baseline[-1])))\n",
    "\n",
    "    return baseline\n",
    "\n",
    "\n",
    "def highpass_filter(a):\n",
    "    cutoff = 2\n",
    "\n",
    "    baseline = gauss_filt(a)\n",
    "    highpass = a - baseline\n",
    "    highpass[highpass < baseline * cutoff] = 0\n",
    "\n",
    "    return highpass\n",
    "\n",
    "\n",
    "def calc_highpass(training_data, similar_moments,\n",
    "                  prediction_window, half_window, method=gauss_filt):\n",
    "    if type(prediction_window) is not timedelta:\n",
    "        prediction_window = timedelta(minutes=prediction_window)\n",
    "\n",
    "    k = len(similar_moments)\n",
    "\n",
    "    similar_data = np.zeros((k, int(prediction_window.total_seconds() / 60) + 2 * half_window))\n",
    "\n",
    "    for i in range(k):\n",
    "        similar_data[i] = training_data[\n",
    "                          similar_moments[i] - timedelta(minutes=half_window)\n",
    "                          : similar_moments[i] + prediction_window + timedelta(minutes=half_window),\n",
    "                          2\n",
    "                          ]\n",
    "\n",
    "    highpass = np.apply_along_axis(highpass_filter, 1, similar_data)\n",
    "\n",
    "    highpass = highpass[:, half_window: -half_window]\n",
    "\n",
    "    w = 3\n",
    "    confidence_threshold = 0.5\n",
    "\n",
    "    paded_highpass = np.pad(highpass, ((0,), (w,)), mode='edge')\n",
    "\n",
    "    highpass_prediction = np.zeros(prediction_window)\n",
    "\n",
    "    for i in range(w, prediction_window + w):\n",
    "        window = paded_highpass[:, i - w:i + w]\n",
    "        confidence = np.count_nonzero(window) / window.size\n",
    "\n",
    "        if confidence > confidence_threshold:\n",
    "            highpass_prediction[\n",
    "                i - w] = np.mean(window[np.nonzero(window)]) * confidence\n",
    "\n",
    "    return highpass_prediction\n",
    "\n",
    "\n",
    "class IEC(object):\n",
    "    \"\"\"The Intelligent Energy Component of a house.\n",
    "    IEC will use several methods to predict the energy consumption of a house\n",
    "    for a given prediction window using historical data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, prediction_window=16 * 60):\n",
    "        \"\"\"Initializing the IEC.\n",
    "\n",
    "        Args:\n",
    "            :param data: Historical Dataset. Last value must be current time\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.now = data.index[-1]\n",
    "        self.prediction_window = prediction_window\n",
    "        self.algorithms = {\n",
    "            \"Simple Mean\": self.simple_mean,\n",
    "            \"Usage Zone Finder\": self.usage_zone_finder,\n",
    "            \"ARIMA\": self.ARIMAforecast,\n",
    "            \"Baseline Finder\": partial(self.baseline_finder, training_window=1440 * 60, k=9, long_interp_range=250,\n",
    "                                       short_interp_range=25, half_window=70, similarity_interval=5,\n",
    "                                       recent_baseline_length=250,\n",
    "                                       observation_length_addition=240, short_term_ease_method=easeOutSine,\n",
    "                                       long_term_ease_method=easeOutCirc),\n",
    "            \"STLF\": self.baseline_finder_dumb,\n",
    "            \"b1\": partial(self.baseline_finder, training_window=1440 * 60, k=9, long_interp_range=250,\n",
    "                          short_interp_range=25, half_window=70, similarity_interval=5, recent_baseline_length=250,\n",
    "                          observation_length_addition=240, short_term_ease_method=easeOutSine,\n",
    "                          long_term_ease_method=easeOutCirc),\n",
    "            \"b2\": partial(self.baseline_finder, training_window=1440 * 60, k=3, long_interp_range=250,\n",
    "                          short_interp_range=25, half_window=70, similarity_interval=5, recent_baseline_length=300,\n",
    "                          observation_length_addition=240, short_term_ease_method=easeOutSine,\n",
    "                          long_term_ease_method=easeOutCirc),\n",
    "            \"b3\": partial(self.baseline_finder, training_window=1440 * 60, k=6, long_interp_range=250,\n",
    "                          short_interp_range=25, half_window=70, similarity_interval=5, recent_baseline_length=200,\n",
    "                          observation_length_addition=240, short_term_ease_method=easeOutSine,\n",
    "                          long_term_ease_method=easeOutCirc),\n",
    "            \"b4\": partial(self.baseline_finder, training_window=1440 * 60, k=12, long_interp_range=250,\n",
    "                          short_interp_range=25, half_window=70, similarity_interval=5, recent_baseline_length=200,\n",
    "                          observation_length_addition=240, short_term_ease_method=easeOutSine,\n",
    "                          long_term_ease_method=easeOutCirc),\n",
    "            \"b5\": partial(self.baseline_finder, training_window=1440 * 60, k=9, long_interp_range=250,\n",
    "                          short_interp_range=25, half_window=50, similarity_interval=5, recent_baseline_length=250,\n",
    "                          observation_length_addition=240, short_term_ease_method=easeOutSine,\n",
    "                          long_term_ease_method=easeOutCirc),\n",
    "            \"b6\": partial(self.baseline_finder, training_window=1440 * 60, k=9, long_interp_range=250,\n",
    "                          short_interp_range=25, half_window=60, similarity_interval=5, recent_baseline_length=300,\n",
    "                          observation_length_addition=240, short_term_ease_method=easeOutSine,\n",
    "                          long_term_ease_method=easeOutCirc),\n",
    "            \"b7\": partial(self.baseline_finder, training_window=1440 * 60, k=9, long_interp_range=250,\n",
    "                          short_interp_range=25, half_window=80, similarity_interval=5, recent_baseline_length=200,\n",
    "                          observation_length_addition=240, short_term_ease_method=easeOutSine,\n",
    "                          long_term_ease_method=easeOutCirc),\n",
    "            \"Gaussian Process\": gaussian_process(self.data, 'Timestamp (ns)', 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ demand (Mean)', kernel='periodic')\n",
    "\n",
    "        }\n",
    "\n",
    "    def predict(self, alg_keys):\n",
    "        index = pd.DatetimeIndex(start=self.now, freq='T', periods=self.prediction_window)\n",
    "        result = pd.DataFrame(index=index)\n",
    "\n",
    "        for key in alg_keys:\n",
    "            r = self.algorithms[key]()\n",
    "            if (r.shape[1] if r.ndim > 1 else 1) > 1:\n",
    "                result[key] = r[:, 0]\n",
    "                result[key + ' STD'] = r[:, 1]\n",
    "            else:\n",
    "                result[key] = r\n",
    "\n",
    "        return result\n",
    "\n",
    "    def simple_mean(self, training_window=24 * 60):\n",
    "        training_data = self.data.tail(training_window)\n",
    "        mean = training_data[cons_col].mean()\n",
    "        return np.repeat(mean, self.prediction_window)\n",
    "\n",
    "    def baseline_finder(self, training_window=1440 * 60, k=7, long_interp_range=300, short_interp_range=15,\n",
    "                        half_window=100, similarity_interval=15, recent_baseline_length=200,\n",
    "                        observation_length_addition=240, short_term_ease_method=easeOutQuad,\n",
    "                        long_term_ease_method=easeOutQuad):\n",
    "        training_data = self.data.tail(training_window)[[cons_col]]\n",
    "\n",
    "        # observation_length is ALL of the current day (till now) + 4 hours\n",
    "        observation_length = mins_in_day(self.now) + observation_length_addition\n",
    "\n",
    "        try:\n",
    "            similar_moments = find_similar_days(\n",
    "                training_data, observation_length, k, similarity_interval, method=baseline_similarity)\n",
    "        except NoSimilarMomentsFound:\n",
    "            # no similar moments were found by our approach.. returning a mean of the last few hours\n",
    "            recent_baseline = training_data[-recent_baseline_length:-1].mean()[cons_col]\n",
    "            baseline = np.repeat(recent_baseline, self.prediction_window)\n",
    "            baseline[0] = training_data.tail(1)[cons_col]\n",
    "            return baseline\n",
    "\n",
    "        baseline = calc_baseline(\n",
    "            training_data, similar_moments, self.prediction_window, half_window, method=gauss_filt, interp_range=0)\n",
    "\n",
    "        # long range interpolate\n",
    "\n",
    "        interp_range = long_interp_range\n",
    "        recent_baseline = training_data[-recent_baseline_length:-1].mean()[cons_col]\n",
    "\n",
    "        method = np.array(list(map(lambda x: long_term_ease_method(x, 0, 1, interp_range), np.arange(interp_range))))\n",
    "\n",
    "        if interp_range > 0:\n",
    "            baseline[:interp_range] = lerp(np.repeat(recent_baseline, interp_range),\n",
    "                                           baseline[:interp_range],\n",
    "                                           method)\n",
    "\n",
    "        # interpolate our prediction from current consumption to predicted\n",
    "        # consumption\n",
    "        # First index is the current time\n",
    "\n",
    "        interp_range = short_interp_range\n",
    "        current_consumption = training_data.tail(1)[cons_col]\n",
    "        method = np.array(list(map(lambda x: short_term_ease_method(x, 0, 1, interp_range), np.arange(interp_range))))\n",
    "        if interp_range > 0:\n",
    "            baseline[:interp_range] = lerp(np.repeat(current_consumption, interp_range),\n",
    "                                           baseline[:interp_range],\n",
    "                                           method)\n",
    "\n",
    "        return baseline[:-1]  # slice last line because we are actually predicting PredictionWindow-1\n",
    "\n",
    "    def baseline_finder_dumb(self, training_window=1440 * 60, k=7):\n",
    "        training_data = self.data.tail(training_window)[[cons_col]]\n",
    "\n",
    "        # observation_length is ALL of the current day (till now) + 4 hours\n",
    "        observation_length = mins_in_day(self.now) + 4 * 60\n",
    "\n",
    "        mse = partial(baseline_similarity, filter=False)\n",
    "\n",
    "        similar_moments = find_similar_days(\n",
    "            training_data, observation_length, k, 60, method=mse)\n",
    "\n",
    "        baseline = calc_baseline_dumb(training_data, similar_moments, self.prediction_window)\n",
    "\n",
    "        # long range interpolate\n",
    "\n",
    "\n",
    "        # interpolate our prediction from current consumption to predicted\n",
    "        # consumption\n",
    "        # First index is the current time\n",
    "\n",
    "        current_consumption = training_data.tail(1)[cons_col]\n",
    "\n",
    "        baseline[0] = current_consumption\n",
    "\n",
    "        return baseline[:-1]  # slice last line because we are actually predicting PredictionWindow-1\n",
    "\n",
    "    def usage_zone_finder(self, training_window=24 * 60 * 120, k=5):\n",
    "\n",
    "        training_data = self.data.tail(training_window)[[cons_col]]\n",
    "\n",
    "        # observation_length is ALL of the current day (till now) + 4 hours\n",
    "        observation_length = mins_in_day(self.now) + (4 * 60)\n",
    "\n",
    "        similar_moments = find_similar_days(\n",
    "            training_data, observation_length, k, 15, method=baseline_similarity)\n",
    "\n",
    "        half_window = 60\n",
    "\n",
    "        baseline = calc_baseline(\n",
    "            training_data, similar_moments, self.prediction_window, half_window, method=gauss_filt)\n",
    "\n",
    "        highpass = calc_highpass(training_data, similar_moments,\n",
    "                                 self.prediction_window, half_window, method=gauss_filt)\n",
    "        final = baseline + highpass\n",
    "\n",
    "        # interpolate our prediction from current consumption to predicted\n",
    "        # consumption\n",
    "        interp_range = 15\n",
    "        # First index is the current time\n",
    "        current_consumption = training_data.tail(1)[cons_col]\n",
    "\n",
    "        final[:interp_range] = lerp(np.repeat(current_consumption, interp_range),\n",
    "                                    final[:interp_range],\n",
    "                                    np.arange(interp_range) / interp_range)\n",
    "\n",
    "        return final[:-1]  # slice last line because we are actually predicting PredictionWindow-1\n",
    "\n",
    "    def ARIMAforecast(self, training_window=1440 * 7, interval=60):\n",
    "        training_data = self.data.tail(training_window)[cons_col].values\n",
    "\n",
    "        TrainingDataIntervals = [sum(training_data[current: current + interval]) / interval for current in\n",
    "                                 range(0, len(training_data), interval)]\n",
    "        # test_stationarity(TrainingDataIntervals)\n",
    "        try:\n",
    "            model = sm.tsa.SARIMAX(TrainingDataIntervals, order=(1, 0, 0),\n",
    "                                   seasonal_order=(1, 1, 1, int(1440 // interval)))\n",
    "            model_fit = model.fit(disp=0)\n",
    "        except:\n",
    "            model = sm.tsa.SARIMAX(TrainingDataIntervals, enforce_stationarity=False)\n",
    "            model_fit = model.fit(disp=0)\n",
    "        #\n",
    "\n",
    "        output = model_fit.forecast(int(self.prediction_window / interval))\n",
    "\n",
    "        # Predictions = np.zeros((self.prediction_window, 4))\n",
    "        # Predictions[:, 0] = np.arange(CurrentUTCTime, CurrentUTCTime + self.prediction_window * 60, 60)\n",
    "        # Predictions[:, 1] = np.arange(CurrentLocalTime, CurrentLocalTime + self.prediction_window * 60, 60)\n",
    "        # Predictions[:, 2] = np.repeat(output, interval)\n",
    "        # Predictions[:, 3] = 0\n",
    "\n",
    "        Predictions = np.repeat(output, interval)\n",
    "        Predictions[0] = training_data[-1]  # current consumption\n",
    "\n",
    "        return Predictions\n",
    "\n",
    "    def gaussian_process(self, df, x_label, y_label, kernel='periodic'):\n",
    "        \"\"\"\n",
    "        Utilize GPy with Different Kernels (Period, etc.) Exponential Kernel for Regression\n",
    "        :param X:    independent variable\n",
    "        :param Y:    depedent variable\n",
    "        :returns:    GPy Model\n",
    "        \"\"\"\n",
    "        kernel = GPy.kern.PeriodicExponential(1)\n",
    "        if kernel is not 'periodic':\n",
    "            #TODO: Include other kernels\n",
    "            pass\n",
    "\n",
    "        X = df[x_label].values.reshape((-1,1))\n",
    "        Y = df[y_label].values.reshape((-1,1))\n",
    "\n",
    "        X = np.nan_to_num(X)\n",
    "        Y = np.nan_to_num(Y)\n",
    "\n",
    "        inds = random.sample(range(1, X.shape[0]),800)\n",
    "\n",
    "        X_inds = X[inds]\n",
    "        Y_inds = Y[inds]\n",
    "\n",
    "        X_samp = (X[inds] - X[inds].mean()) /(X[inds].std())\n",
    "        Y_samp = (Y[inds] - Y[inds].mean()) /(Y[inds].std())\n",
    "\n",
    "        gpy_model = GPy.models.GPRegression(X_samp,Y_samp,kernel)\n",
    "        gpy_model.optimize(messages=True)\n",
    "\n",
    "        return gpy_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our Data from pmu_data\n",
    "csv_filename = '../pmu_data.csv'\n",
    "df = pd.read_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp (ns)',\n",
       " 'Human-Readable Time (UTC)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ demand (Min)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ demand (Mean)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ demand (Max)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ demand (Count)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ price (Min)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ price (Mean)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ price (Max)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ price (Count)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ summation_received (Min)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ summation_received (Mean)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ summation_received (Max)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ summation_received (Count)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ summation_delivered (Min)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ summation_delivered (Mean)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ summation_delivered (Max)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ summation_delivered (Count)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ tier (Min)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ tier (Mean)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ tier (Max)',\n",
       " 'ciee/ devices/ meter/ 0xd8d5b9000000a110/ tier (Count)']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Timestamp (ns)',\n",
       " 'Human-Readable Time (UTC)',\n",
       " 'demand (Min)',\n",
       " 'demand (Mean)',\n",
       " 'demand (Max)',\n",
       " 'demand (Count)',\n",
       " 'price (Min)',\n",
       " 'price (Mean)',\n",
       " 'price (Max)',\n",
       " 'price (Count)',\n",
       " 'summation_received (Min)',\n",
       " 'summation_received (Mean)',\n",
       " 'summation_received (Max)',\n",
       " 'summation_received (Count)',\n",
       " 'summation_delivered (Min)',\n",
       " 'summation_delivered (Mean)',\n",
       " 'summation_delivered (Max)',\n",
       " 'summation_delivered (Count)',\n",
       " 'tier (Min)',\n",
       " 'tier (Mean)',\n",
       " 'tier (Max)',\n",
       " 'tier (Count)']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Human-Readable Time (UTC)'] = df['Human-Readable Time (UTC)'].apply(pd.to_datetime)\n",
    "\n",
    "def shorten_title(text, target_character='/'):\n",
    "    start_idx = text.rfind(target_character) + 1\n",
    "    if start_idx:\n",
    "        return text[start_idx:].strip()\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df = df.rename({col_name: shorten_title(col_name) for col_name in df.columns.values}, axis='columns')\n",
    "list(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GPy.kern.PeriodicExponential(1)\n",
    "\n",
    "X = df['Timestamp (ns)'].values.reshape((-1,1))\n",
    "Y = df['demand (Mean)'].values.reshape((-1,1))\n",
    "\n",
    "X=np.nan_to_num(X)\n",
    "Y= np.nan_to_num(Y)\n",
    "\n",
    "inds = random.sample(range(1, X.shape[0]),800)\n",
    "\n",
    "X_inds= X[inds]\n",
    "Y_inds = Y[inds]\n",
    "\n",
    "X_samp = (X[inds] - X[inds].mean()) /(X[inds].std())\n",
    "Y_samp = (Y[inds] - Y[inds].mean()) /(Y[inds].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running L-BFGS-B (Scipy implementation) Code:\n",
      "  runtime   i      f              |g|        \n",
      "    00s19  0003   1.073787e+03   1.725233e+02 \n",
      "    02s23  0039   1.017257e+03   3.346129e-01 \n",
      "    04s35  0077   1.017224e+03   1.197362e-03 \n",
      "Runtime:     04s35\n",
      "Optimization status: Converged\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       ".pd{\n",
       "    font-family: \"Courier New\", Courier, monospace !important;\n",
       "    width: 100%;\n",
       "    padding: 3px;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<p class=pd>\n",
       "<b>Model</b>: GP regression<br>\n",
       "<b>Objective</b>: 1017.2244699411794<br>\n",
       "<b>Number of Parameters</b>: 4<br>\n",
       "<b>Number of Optimization Parameters</b>: 4<br>\n",
       "<b>Updates</b>: True<br>\n",
       "</p>\n",
       "<style type=\"text/css\">\n",
       ".tg  {font-family:\"Courier New\", Courier, monospace !important;padding:2px 3px;word-break:normal;border-collapse:collapse;border-spacing:0;border-color:#DCDCDC;margin:0px auto;width:100%;}\n",
       ".tg td{font-family:\"Courier New\", Courier, monospace !important;font-weight:bold;color:#444;background-color:#F7FDFA;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg th{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;color:#fff;background-color:#26ADE4;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;}\n",
       ".tg .tg-left{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:left;}\n",
       ".tg .tg-center{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:center;}\n",
       ".tg .tg-right{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:right;}\n",
       "</style>\n",
       "<table class=\"tg\"><tr><th><b>  GP_regression.                  </b></th><th><b>           value</b></th><th><b>constraints</b></th><th><b>priors</b></th></tr>\n",
       "<tr><td class=tg-left>  periodic_exponential.variance   </td><td class=tg-right>   17.4331244121</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "<tr><td class=tg-left>  periodic_exponential.lengthscale</td><td class=tg-right>0.00479247045633</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "<tr><td class=tg-left>  periodic_exponential.period     </td><td class=tg-right>   4.58429561512</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "<tr><td class=tg-left>  Gaussian_noise.variance         </td><td class=tg-right>   0.69962203493</td><td class=tg-center>    +ve    </td><td class=tg-center>      </td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<GPy.models.gp_regression.GPRegression at 0x1105f3a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = GPy.models.GPRegression(X_samp,Y_samp,kernel)\n",
    "m.optimize(messages=True)\n",
    "from IPython.display import display\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GPy.kern.PeriodicExponential(1)\n",
    "\n",
    "X = df['Timestamp (ns)'].values.reshape((-1,1))\n",
    "Y = df['demand (Mean)', 'price (Mean)', 'tier (Mean)'].values.reshape((-1,3))\n",
    "\n",
    "X=np.nan_to_num(X)\n",
    "Y= np.nan_to_num(Y)\n",
    "\n",
    "inds = random.sample(range(1, X.shape[0]),800)\n",
    "\n",
    "X_inds= X[inds]\n",
    "Y_inds = Y[inds]\n",
    "\n",
    "X_samp = (X[inds] - X[inds].mean()) /(X[inds].std())\n",
    "Y_samp = (Y[inds] - Y[inds].mean()) /(Y[inds].std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
